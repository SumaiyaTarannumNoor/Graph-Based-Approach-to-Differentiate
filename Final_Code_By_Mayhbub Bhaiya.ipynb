{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cAjQXl7a8dq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import spacy\n",
        "import networkx as nx\n",
        "\n",
        "from bangla_stemmer.stemmer import stemmer\n",
        "\n",
        "human_dir = \"dataset/bn_human_vs_ai_corpus/human_written\"\n",
        "llm_dir = \"dataset/bn_human_vs_ai_corpus/llm_generated\"\n",
        "nlp = spacy.blank(\"bn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLvu1QoKa8dt"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNtYQneaa8du"
      },
      "outputs": [],
      "source": [
        "# Bengali stop words\n",
        "bengali_stopwords = set([\n",
        "    \"এবং\", \"তাহা\", \"উপর\", \"হয়\", \"করে\", \"যায়\", \"হতে\", \"এই\", \"তা\", \"তার\", \"হয়েছে\", \"ছিল\",\n",
        "    \"কিন্তু\", \"তাকে\", \"আমরা\", \"আপনি\", \"তাদের\", \"সব\", \"অনেক\", \"কিছু\", \"কখনো\", \"একটি\", \"এটি\",\n",
        "])\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, digits, and non-Bengali characters\n",
        "    text = re.sub(r\"[^\\u0980-\\u09FF\\s]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    stmr = stemmer.BanglaStemmer()\n",
        "    tokens = [stmr.stem(word) for word in tokens]\n",
        "    tokens = [token for token in tokens if token not in bengali_stopwords]\n",
        "    return tokens\n",
        "\n",
        "# Load text data\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "                texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "# Load data\n",
        "human_texts = load_texts(human_dir)\n",
        "llm_texts = load_texts(llm_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrOlXeWZa8dv"
      },
      "source": [
        "Train Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qTtI2Wpa8dv"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec models\n",
        "def train_word2vec(texts, vector_size=100, window=5, min_count=2):\n",
        "    tokenized_texts = [preprocess_text(text) for text in texts]\n",
        "    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
        "    return model\n",
        "\n",
        "human_w2v_model = train_word2vec(human_texts)\n",
        "llm_w2v_model = train_word2vec(llm_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjt2L_aqa8dw"
      },
      "source": [
        "Extract Word2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAFhKa3Ea8dw"
      },
      "outputs": [],
      "source": [
        "# Document embedding using Word2Vec\n",
        "def get_document_embedding(tokens, model):\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) > 0:\n",
        "        return np.mean(word_vectors, axis=0)  # Average word embeddings\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)    # Default to zero vector\n",
        "\n",
        "def extract_word2vec_features(text, human_model, llm_model):\n",
        "    tokens = preprocess_text(text)\n",
        "    human_emb = get_document_embedding(tokens, human_model)\n",
        "    llm_emb = get_document_embedding(tokens, llm_model)\n",
        "    return np.concatenate([human_emb, llm_emb])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMZPBEVva8dw"
      },
      "source": [
        "Build and Extract Syntactic Graph Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD6SgXzca8dx"
      },
      "outputs": [],
      "source": [
        "# Build ISG for a document\n",
        "def build_isg(text):\n",
        "    doc = nlp(text)\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for token in doc:\n",
        "        G.add_node(token.text, pos=token.pos_, dep=token.dep_)\n",
        "        if token.head != token:\n",
        "            G.add_edge(token.head.text, token.text, relation=token.dep_)\n",
        "\n",
        "    return G\n",
        "\n",
        "# # Extract graph features\n",
        "# def extract_graph_features(G):\n",
        "#     features = {}\n",
        "#     degree_sequence = [G.degree(n) for n in G.nodes()]\n",
        "#     features['avg_degree'] = np.mean(degree_sequence) if degree_sequence else 0\n",
        "#     features['max_degree'] = np.max(degree_sequence) if degree_sequence else 0\n",
        "#     features['min_degree'] = np.min(degree_sequence) if degree_sequence else 0\n",
        "#     features['edge_density'] = nx.density(G) if len(G.nodes()) > 0 else 0\n",
        "#     features['clustering_coefficient'] = nx.average_clustering(G) if len(G.nodes()) > 0 else 0\n",
        "#     return list(features.values())\n",
        "\n",
        "\n",
        "# Function to compute the H-index based on node degrees\n",
        "def h_index(G):\n",
        "    degrees = sorted([d for n, d in G.degree()], reverse=True)\n",
        "    h = 0\n",
        "    for i, degree in enumerate(degrees):\n",
        "        if degree >= i + 1:\n",
        "            h = i + 1\n",
        "        else:\n",
        "            break\n",
        "    return h\n",
        "\n",
        "# Extract graph features\n",
        "def extract_graph_features(G):\n",
        "\n",
        "    G = G.to_undirected()\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "    features = {}\n",
        "    degree_sequence = [G.degree(n) for n in G.nodes()]\n",
        "\n",
        "    # Degree features\n",
        "    features['avg_degree'] = np.mean(degree_sequence) if degree_sequence else 0\n",
        "    features['max_degree'] = np.max(degree_sequence) if degree_sequence else 0\n",
        "    features['min_degree'] = np.min(degree_sequence) if degree_sequence else 0\n",
        "\n",
        "    # Graph properties\n",
        "    features['num_edges'] = G.number_of_edges()\n",
        "    features['density'] = nx.density(G) if len(G.nodes()) > 0 else 0\n",
        "    features['radius'] = nx.radius(G) if nx.is_connected(G) else -1\n",
        "    features['diameter'] = nx.diameter(G) if nx.is_connected(G) else -1\n",
        "    features['circumference'] = nx.circumference(G) if nx.is_connected(G) else -1\n",
        "    features['girth'] = nx.girth(G) if nx.is_connected(G) else -1\n",
        "\n",
        "    # features['vertex_connectivity'] = nx.number_connected_components(G) if nx.is_connected(G) else -1\n",
        "    # features['edge_connectivity'] = nx.edge_connectivity(G) if nx.is_connected(G) else -1\n",
        "\n",
        "    # Chromatic features\n",
        "    features['chromatic_number'] = len(set(nx.coloring.greedy_color(G).values())) if len(G.nodes()) > 0 else -1\n",
        "    # # features['chromatic_index'] = nx.chromatic_index(G) if len(G.nodes()) > 0 else -1\n",
        "\n",
        "    # # Clique number\n",
        "    # features['clique_number'] = nx.graph_clique_number(G) if len(G.nodes()) > 0 else -1\n",
        "\n",
        "    # Other graph properties\n",
        "    features['degeneracy'] = max(nx.core_number(G).values()) if len(G.nodes()) > 0 else -1\n",
        "    features['clustering_coeff'] = nx.average_clustering(G) if len(G.nodes()) > 0 else 0\n",
        "    features['global_clustering_coeff'] = nx.transitivity(G) if len(G.nodes()) > 0 else 0\n",
        "    features['h_index'] = h_index(G) if len(G.nodes()) > 0 else -1\n",
        "\n",
        "    return list(features.values())\n",
        "\n",
        "def extract_features_with_graph(text, human_model, llm_model):\n",
        "    # Word2Vec features\n",
        "    w2v_features = extract_word2vec_features(text, human_model, llm_model)\n",
        "    # Graph features\n",
        "    G = build_isg(text)\n",
        "    graph_features = extract_graph_features(G)\n",
        "    # Combine features\n",
        "    return np.concatenate([w2v_features, graph_features])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDbpNt9la8dx"
      },
      "source": [
        "Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq2-w-N6a8dx"
      },
      "outputs": [],
      "source": [
        "# Extract features for all texts\n",
        "human_features = [extract_features_with_graph(text, human_w2v_model, llm_w2v_model) for text in human_texts]\n",
        "llm_features = [extract_features_with_graph(text, human_w2v_model, llm_w2v_model) for text in llm_texts]\n",
        "\n",
        "# Labels: 0 = Human, 1 = LLM\n",
        "human_labels = [0] * len(human_features)\n",
        "llm_labels = [1] * len(llm_features)\n",
        "\n",
        "# Combine features and labels\n",
        "features = np.array(human_features + llm_features)\n",
        "labels = np.array(human_labels + llm_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UugnLAURa8dy"
      },
      "source": [
        "Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQPBppsma8dy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCCHtYSpa8dy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=[\"Human-written\", \"LLM-generated\"]):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPVFIYHoa8dy"
      },
      "outputs": [],
      "source": [
        "# Classify a new document\n",
        "def classify_new_document(text, classifier, human_model, llm_model):\n",
        "    features = extract_features_with_graph(text, human_model, llm_model)\n",
        "    prediction = classifier.predict([features])\n",
        "    return \"Human-written\" if prediction[0] == 0 else \"LLM-generated\"\n",
        "\n",
        "# Function to classify a single txt file\n",
        "def classify_single_document(file_path, classifier, human_model, llm_model):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    result = classify_new_document(text, classifier, human_model, llm_model)\n",
        "    print(f\"File: {file_path} - Classification: {result}\")\n",
        "\n",
        "# Define the file path to the txt file\n",
        "file_path = \"dataset/bn_human_vs_ai_corpus/llm_generated/shapure_choto_golpo_ai.txt\"  # Replace with the actual file path\n",
        "\n",
        "# Call the function to classify the document\n",
        "classify_single_document(file_path, clf, human_w2v_model, llm_w2v_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TN8uvya8dy"
      },
      "source": [
        "ISG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIK-6VwVa8dz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from bangla_stemmer.stemmer import stemmer\n",
        "\n",
        "# Load spaCy Bengali NLP model\n",
        "nlp = spacy.blank(\"bn\")  # Ensure you have the Bengali spaCy model\n",
        "\n",
        "# Bengali stop word list (extend or replace with a more comprehensive list if needed)\n",
        "bengali_stopwords = set([\n",
        "    \"এবং\", \"তাহা\", \"উপর\", \"হয়\", \"করে\", \"যায়\", \"হতে\", \"এই\", \"তা\", \"তার\", \"হয়েছে\", \"ছিল\",\n",
        "    \"কিন্তু\", \"তাকে\", \"আমরা\", \"আপনি\", \"তাদের\", \"সব\", \"অনেক\", \"কিছু\", \"কখনো\", \"একটি\", \"এটি\",\n",
        "])\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, digits, and non-Bengali characters\n",
        "    text = re.sub(r\"[^\\u0980-\\u09FF\\s]\", \"\", text)  # Keep only Bengali characters and whitespace\n",
        "\n",
        "    tokens = text.split()\n",
        "    print(tokens)\n",
        "    stmr = stemmer.BanglaStemmer()\n",
        "    tokens = [stmr.stem(word) for word in tokens]\n",
        "\n",
        "    # Remove stopwords and return cleaned tokens\n",
        "    tokens = [token for token in tokens if token not in bengali_stopwords]\n",
        "    return tokens\n",
        "\n",
        "# Build Integrated Syntactic Graph (ISG)\n",
        "def build_isg(cleaned_tokens):\n",
        "    # Initialize a directed graph for the ISG\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes and edges based on token position (basic syntactic structure example)\n",
        "    for i, token in enumerate(cleaned_tokens):\n",
        "        G.add_node(token, position=i)  # Add token as a node with position as an attribute\n",
        "        if i > 0:\n",
        "            G.add_edge(cleaned_tokens[i - 1], token)  # Add edges between consecutive tokens\n",
        "\n",
        "    return G\n",
        "\n",
        "# Plot the graph with cleaner visualization\n",
        "def plot_graph(G, title=\"Graph Visualization\", max_nodes=500, weight_threshold=None):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # If the graph is too large, use a subgraph\n",
        "    if len(G.nodes) > max_nodes:\n",
        "        nodes_to_keep = list(G.nodes)[:max_nodes]\n",
        "        G = G.subgraph(nodes_to_keep)\n",
        "\n",
        "    # Filter edges by weight (if threshold is provided and graph is weighted)\n",
        "    if weight_threshold is not None and nx.get_edge_attributes(G, 'weight'):\n",
        "        edges_to_keep = [(u, v) for u, v, d in G.edges(data=True) if d.get('weight', 0) > weight_threshold]\n",
        "        G = G.edge_subgraph(edges_to_keep)\n",
        "\n",
        "    # Positioning algorithm\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    # Draw the graph\n",
        "    nx.draw(\n",
        "        G, pos,\n",
        "        with_labels=False,\n",
        "        node_color=\"skyblue\",\n",
        "        node_size=50,  # Smaller nodes\n",
        "        edge_color=\"gray\",  # Light-colored edges\n",
        "        width=0.5,  # Thinner edges\n",
        "        alpha=0.7  # Add transparency\n",
        "    )\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# # Print graph properties\n",
        "# def print_graph_properties(G):\n",
        "#     print(f\"Graph Properties:\")\n",
        "#     print(f\"- Number of nodes: {G.number_of_nodes()}\")\n",
        "#     print(f\"- Number of edges: {G.number_of_edges()}\")\n",
        "#     print(f\"- Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
        "#     print(f\"- Is graph connected? {'Yes' if nx.is_connected(G.to_undirected()) else 'No'}\")\n",
        "#     print(f\"- Diameter (if connected): {nx.diameter(G.to_undirected()) if nx.is_connected(G.to_undirected()) else 'N/A'}\")\n",
        "#     print()\n",
        "\n",
        "\n",
        "# Function to compute the H-index based on node degrees\n",
        "def h_index(G):\n",
        "    degrees = sorted([d for n, d in G.degree()], reverse=True)\n",
        "    h = 0\n",
        "    for i, degree in enumerate(degrees):\n",
        "        if degree >= i + 1:\n",
        "            h = i + 1\n",
        "        else:\n",
        "            break\n",
        "    return h\n",
        "\n",
        "# Print graph properties\n",
        "def print_graph_properties(G):\n",
        "    print(f\"Graph Properties:\")\n",
        "    print(f\"- Number of nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"- Number of edges: {G.number_of_edges()}\")\n",
        "    print(f\"- Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
        "\n",
        "    # Convert to undirected graph for connectivity checks and related properties\n",
        "    G_undirected = G.to_undirected()\n",
        "    G_undirected.remove_edges_from(nx.selfloop_edges(G_undirected))\n",
        "\n",
        "    print(f\"- Is graph connected? {'Yes' if nx.is_connected(G_undirected) else 'No'}\")\n",
        "    print(f\"- Diameter (if connected): {nx.diameter(G_undirected) if nx.is_connected(G_undirected) else 'N/A'}\")\n",
        "    print(f\"- Radius (if connected): {nx.radius(G_undirected) if nx.is_connected(G_undirected) else 'N/A'}\")\n",
        "    print(f\"- Density: {nx.density(G_undirected):.4f}\")\n",
        "    print(f\"- Clustering coefficient: {nx.average_clustering(G_undirected):.4f}\")\n",
        "    print(f\"- Girth (if connected): {nx.girth(G_undirected) if nx.is_connected(G_undirected) else 'N/A'}\")\n",
        "    print(f\"- Vertex connectivity: {nx.number_connected_components(G_undirected) if nx.is_connected(G_undirected) else 'N/A'}\")\n",
        "    print(f\"- Edge connectivity: {nx.edge_connectivity(G_undirected) if nx.is_connected(G_undirected) else 'N/A'}\")\n",
        "    print(f\"- Degeneracy: {max(nx.core_number(G_undirected).values()) if len(G_undirected.nodes()) > 0 else 'N/A'}\")\n",
        "    print(f\"- Global clustering coefficient: {nx.transitivity(G_undirected) if len(G_undirected.nodes()) > 0 else 'N/A'}\")\n",
        "    print(f\"- H-index: {h_index(G_undirected) if len(G_undirected.nodes()) > 0 else 'N/A'}\")\n",
        "    print()\n",
        "\n",
        "# Process all text files in a given folder\n",
        "def process_folder(folder_path, label):\n",
        "    graphs = []\n",
        "    print(f\"Processing {label} files in folder: {folder_path}\\n\")\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read()\n",
        "\n",
        "                # Preprocess text\n",
        "                cleaned_tokens = preprocess_text(text)\n",
        "                print(f\"File: {filename}\")\n",
        "                print(f\"Cleaned tokens: {cleaned_tokens}\\n\")\n",
        "\n",
        "                # Build graph\n",
        "                G = build_isg(cleaned_tokens)\n",
        "                graphs.append((filename, G))\n",
        "\n",
        "                # Plot graph\n",
        "                plot_graph(G, title=f\"Graph for {label}: {filename}\")\n",
        "\n",
        "                # Print graph properties\n",
        "                print(f\"Graph properties for {filename}:\")\n",
        "                print_graph_properties(G)\n",
        "    return graphs\n",
        "\n",
        "# Main folder structure\n",
        "base_folder = \"dataset/bn_human_vs_ai_corpus\"\n",
        "human_folder = os.path.join(base_folder, \"human_written\")\n",
        "llm_folder = os.path.join(base_folder, \"llm_generated\")\n",
        "\n",
        "# Process human-written and LLM-generated text files\n",
        "human_graphs = process_folder(human_folder, label=\"Human-written\")\n",
        "llm_graphs = process_folder(llm_folder, label=\"LLM-generated\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}